# Copyright 2017 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# NOTE: This class is auto generated by the swagger code generator program.
# https://github.com/swagger-api/swagger-codegen.git
# Do not edit the class manually.

defmodule GoogleApi.Dataproc.V1.Model.OrderedJob do
  @moduledoc """
  A job executed by the workflow.

  ## Attributes

  - hadoopJob (HadoopJob): Job is a Hadoop job. Defaults to: `null`.
  - hiveJob (HiveJob): Job is a Hive job. Defaults to: `null`.
  - labels (%{optional(String.t) &#x3D;&gt; String.t}): Optional. The labels to associate with this job.Label keys must be between 1 and 63 characters long, and must conform to the following regular expression: \\p{Ll}\\p{Lo}{0,62}Label values must be between 1 and 63 characters long, and must conform to the following regular expression: \\p{Ll}\\p{Lo}\\p{N}_-{0,63}No more than 32 labels can be associated with a given job. Defaults to: `null`.
  - pigJob (PigJob): Job is a Pig job. Defaults to: `null`.
  - prerequisiteStepIds ([String.t]): Optional. The optional list of prerequisite job step_ids. If not specified, the job will start at the beginning of workflow. Defaults to: `null`.
  - pysparkJob (PySparkJob): Job is a Pyspark job. Defaults to: `null`.
  - scheduling (JobScheduling): Optional. Job scheduling configuration. Defaults to: `null`.
  - sparkJob (SparkJob): Job is a Spark job. Defaults to: `null`.
  - sparkSqlJob (SparkSqlJob): Job is a SparkSql job. Defaults to: `null`.
  - stepId (String.t): Required. The step id. The id must be unique among all jobs within the template.The step id is used as prefix for job id, as job goog-dataproc-workflow-step-id label, and in prerequisiteStepIds field from other steps.The id must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), and hyphens (-). Cannot begin or end with underscore or hyphen. Must consist of between 3 and 50 characters. Defaults to: `null`.
  """

  use GoogleApi.Gax.ModelBase

  @type t :: %__MODULE__{
          :hadoopJob => GoogleApi.Dataproc.V1.Model.HadoopJob.t(),
          :hiveJob => GoogleApi.Dataproc.V1.Model.HiveJob.t(),
          :labels => map(),
          :pigJob => GoogleApi.Dataproc.V1.Model.PigJob.t(),
          :prerequisiteStepIds => list(any()),
          :pysparkJob => GoogleApi.Dataproc.V1.Model.PySparkJob.t(),
          :scheduling => GoogleApi.Dataproc.V1.Model.JobScheduling.t(),
          :sparkJob => GoogleApi.Dataproc.V1.Model.SparkJob.t(),
          :sparkSqlJob => GoogleApi.Dataproc.V1.Model.SparkSqlJob.t(),
          :stepId => any()
        }

  field(:hadoopJob, as: GoogleApi.Dataproc.V1.Model.HadoopJob)
  field(:hiveJob, as: GoogleApi.Dataproc.V1.Model.HiveJob)
  field(:labels, type: :map)
  field(:pigJob, as: GoogleApi.Dataproc.V1.Model.PigJob)
  field(:prerequisiteStepIds, type: :list)
  field(:pysparkJob, as: GoogleApi.Dataproc.V1.Model.PySparkJob)
  field(:scheduling, as: GoogleApi.Dataproc.V1.Model.JobScheduling)
  field(:sparkJob, as: GoogleApi.Dataproc.V1.Model.SparkJob)
  field(:sparkSqlJob, as: GoogleApi.Dataproc.V1.Model.SparkSqlJob)
  field(:stepId)
end

defimpl Poison.Decoder, for: GoogleApi.Dataproc.V1.Model.OrderedJob do
  def decode(value, options) do
    GoogleApi.Dataproc.V1.Model.OrderedJob.decode(value, options)
  end
end

defimpl Poison.Encoder, for: GoogleApi.Dataproc.V1.Model.OrderedJob do
  def encode(value, options) do
    GoogleApi.Gax.ModelBase.encode(value, options)
  end
end
